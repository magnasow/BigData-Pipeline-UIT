from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# ğŸ”¹ Initialisation de la session Spark
spark = SparkSession.builder \
    .appName("NettoyageConnectivityUIT") \
    .getOrCreate()

# ğŸ” Configuration MinIO (accÃ¨s S3-compatible)
hadoop_conf = spark._jsc.hadoopConfiguration()
hadoop_conf.set("fs.s3a.access.key", "minioadmin")
hadoop_conf.set("fs.s3a.secret.key", "minioadmin")
hadoop_conf.set("fs.s3a.endpoint", "http://minio:9000")
hadoop_conf.set("fs.s3a.path.style.access", "true")
hadoop_conf.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

# ğŸ“¥ Lecture du fichier CSV depuis le volume montÃ© (/data)
input_path = "/data/connectivite_mobile_internet.csv"
df = spark.read.option("header", "true").csv(input_path)

# ğŸ§¼ Nettoyage simple : suppression des lignes avec des valeurs manquantes
df_cleaned = df.dropna()

# (Optionnel) Exemple de transformation supplÃ©mentaire
# df_cleaned = df_cleaned.withColumnRenamed("OldColumn", "NewColumn")

# ğŸ“¤ Ã‰criture en format Parquet dans le bucket MinIO (S3A)
output_path = "s3a://bigdata-pipeline/connectivite_mobile_internet.parquet"
df_cleaned.write.mode("overwrite").parquet(output_path)

print(f"âœ… Parquet sauvegardÃ© dans MinIO : {output_path}")

# ğŸ›‘ ArrÃªt de la session Spark
spark.stop()
